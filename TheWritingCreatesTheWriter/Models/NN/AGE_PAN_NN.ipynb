{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PAN_nn.ipynb","provenance":[{"file_id":"1cb_A_GmdfkWop0NThtKZsJIi_Q1-wgnL","timestamp":1613121008421}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"JRBaXfd5Zu3y"},"source":["# Pan nn Text Classification Test\r\n","Trying to classify the age of an author based on a written conversation\r\n","\r\n","## 1. Import libraries"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mHHvR_ghSOpH","executionInfo":{"status":"ok","timestamp":1615736881869,"user_tz":-60,"elapsed":3553,"user":{"displayName":"Emma Qusit","photoUrl":"","userId":"00282587743940963113"}},"outputId":"80ad1b9e-6a34-4490-9bf5-4848f4c1d3e5"},"source":["!pip install torchtext==0.4"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torchtext==0.4 in /usr/local/lib/python3.7/dist-packages (0.4.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4) (2.23.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4) (1.8.0+cu101)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4) (1.15.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.4) (4.41.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4) (2020.12.5)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.4) (2.10)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->torchtext==0.4) (3.7.4.3)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AwGxDCFTNGF2","executionInfo":{"status":"ok","timestamp":1615736887692,"user_tz":-60,"elapsed":9368,"user":{"displayName":"Emma Qusit","photoUrl":"","userId":"00282587743940963113"}},"outputId":"ea605e82-69ce-4c69-b316-aa564e0aa08d"},"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import pandas as pd\n","import numpy as np\n","import re\n","import os\n","import time\n","\n","import torch\n","import torchtext\n","\n","from torchtext.datasets import text_classification\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from torch.utils.data.dataset import random_split\n","import nltk\n","import plotly.express as px\n","from nltk.stem import WordNetLemmatizer\n","import plotly.graph_objs as go\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","\n","from collections import defaultdict, OrderedDict, Counter\n","import operator"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VP1rrVDBuGkQ"},"source":["## 2. Load and preprocces the dataframe"]},{"cell_type":"code","metadata":{"id":"BrW8KP_mNPkj","executionInfo":{"status":"ok","timestamp":1615736968892,"user_tz":-60,"elapsed":11402,"user":{"displayName":"Emma Qusit","photoUrl":"","userId":"00282587743940963113"}}},"source":["df = pd.read_csv('/content/drive/MyDrive/Language, speech and dialogue processing/Datasets/df_pan_train.csv') "],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ll3zoy2sNR1T","executionInfo":{"status":"ok","timestamp":1615736978518,"user_tz":-60,"elapsed":530,"user":{"displayName":"Emma Qusit","photoUrl":"","userId":"00282587743940963113"}}},"source":["df = df.drop(['Unnamed: 0', 'lang', 'id'], axis=1)\r\n"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"BdDOi2IThlDd","executionInfo":{"status":"ok","timestamp":1615736928049,"user_tz":-60,"elapsed":49714,"user":{"displayName":"Emma Qusit","photoUrl":"","userId":"00282587743940963113"}}},"source":["df_sub = df[:10]\n","df_sub.to_csv('/content/drive/MyDrive/Language, speech and dialogue processing/sub_PAN.csv')"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"GE6EoN5DBqTF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1615736928520,"user_tz":-60,"elapsed":50182,"user":{"displayName":"Emma Qusit","photoUrl":"","userId":"00282587743940963113"}},"outputId":"37fc77e3-5b85-4d5f-c589-043575c35285"},"source":["ages = df[\"age_group\"].unique().tolist()\n","ages.sort()\n","age_dict = {}\n","for age in ages:\n","  age_dict[age] = df.loc[df['age_group'] == age]\n","\n","for key, value in age_dict.items():\n","  age_dict[key] = value\n","  \n","print(age_dict)"],"execution_count":7,"outputs":[{"output_type":"stream","text":["{'10s':        age_group  gender                                               text\n","20           10s    male  Instantly you will notice people with the purc...\n","21           10s    male  Too many people throw in the towel early on be...\n","29           10s  female  he Best Site for Free Money Offers and Competi...\n","31           10s  female  No make a difference what your good reasons we...\n","32           10s  female  <strong>;Salam.\\n<br />;<br />;okay, first, I ...\n","...          ...     ...                                                ...\n","401933       10s    male  You want to fix your PS3 and that's it. Anothe...\n","401934       10s  female  In recent decades, there are many game systems...\n","401935       10s  female  Super Mario brothers 3 is another major game w...\n","401959       10s  female  Men and women through every wander of life are...\n","401979       10s    male  I LIKE SURFING AND NEW! ADD ME AND WE CAN BECO...\n","\n","[27428 rows x 3 columns], '20s':        age_group  gender                                               text\n","0            20s  female  The utilization of this item of therapy gear i...\n","1            20s  female  Before, an individual additional learn about t...\n","4            20s  female  Based in Southwest Louisiana, the Law Office o...\n","10           20s    male  There are marriage ceremony band sets that won...\n","11           20s    male  As with the greater part of metals, with perha...\n","...          ...     ...                                                ...\n","401983       20s    male  Organizers are boosting supplies of water and ...\n","401984       20s    male  Most a few other different choices are around ...\n","401989       20s    male  Microsoft Office 2010 is the newest model (as ...\n","401990       20s    male  Prior to beginning the set up process of the a...\n","401991       20s    male  Prior to starting up the installation process ...\n","\n","[149443 rows x 3 columns], '30s':        age_group  gender                                               text\n","2            30s    male  The vending device organization is one particu...\n","3            30s    male  National Treasure - three Stars (Excellent)<br...\n","5            30s    male  In regards to bee pollen benefits you have pro...\n","6            30s    male  Online advertising is a method of promotion th...\n","7            30s  female  As you stroll by the neighborhood, you will lo...\n","...          ...     ...                                                ...\n","401985       30s  female  A non-public college in Brazil mentioned Tuesd...\n","401986       30s  female  The makeup and splendor schedule that you sele...\n","401987       30s  female  Making corsets is actually a fun and rewarding...\n","401988       30s    male  <a href=\"http://en.pan.netcom/go/out/url=-aHR0...\n","401992       30s  female  Hello. My name is Angelique'. I am 43yrs young...\n","\n","[225122 rows x 3 columns]}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"UIQxrhL2F_eG","executionInfo":{"status":"ok","timestamp":1615736928521,"user_tz":-60,"elapsed":50182,"user":{"displayName":"Emma Qusit","photoUrl":"","userId":"00282587743940963113"}}},"source":["\n","\n","dataframes_10s = []\n","dataframes_20s = []\n","dataframes_30s = []\n","for key, value in age_dict.items():\n","  if key == '10s':\n","    dataframes_10s.append(value)\n","  elif key == '20s':\n","    dataframes_20s.append(value)\n","  elif '30s':\n","    dataframes_30s.append(value)\n","\n","df_10s = pd.concat(dataframes_10s)\n","df_20s = pd.concat(dataframes_20s)\n","df_30s = pd.concat(dataframes_30s)\n","\n","all_dataframes = [df_10s, df_20s, df_30s]\n","\n","min_len = len(all_dataframes[0])\n","\n","for df in all_dataframes:\n","  if len(df) < min_len:\n","    min_len = len(df)\n","\n","df_10s[\"age\"] = 0\n","df_20s[\"age\"] = 1\n","df_30s[\"age\"] = 2\n","\n","all_dataframes = [df_10s.sample(min_len), df_20s.sample(min_len), df_30s.sample(min_len)]\n","\n","df = pd.concat(all_dataframes)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"eoOXbeKhvSRs","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1615737005902,"user_tz":-60,"elapsed":809,"user":{"displayName":"Emma Qusit","photoUrl":"","userId":"00282587743940963113"}},"outputId":"2e7ac706-db12-49b0-b226-7dfd0364234b"},"source":[""],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>age_group</th>\n","      <th>gender</th>\n","      <th>text</th>\n","      <th>encoded_gender</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>20s</td>\n","      <td>female</td>\n","      <td>The utilization of this item of therapy gear i...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>20s</td>\n","      <td>female</td>\n","      <td>Before, an individual additional learn about t...</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>30s</td>\n","      <td>male</td>\n","      <td>The vending device organization is one particu...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>30s</td>\n","      <td>male</td>\n","      <td>National Treasure - three Stars (Excellent)&lt;br...</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>20s</td>\n","      <td>female</td>\n","      <td>Based in Southwest Louisiana, the Law Office o...</td>\n","      <td>0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  age_group  ... encoded_gender\n","0       20s  ...              0\n","1       20s  ...              0\n","2       30s  ...              1\n","3       30s  ...              1\n","4       20s  ...              0\n","\n","[5 rows x 4 columns]"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P0nTy97oa-4u","executionInfo":{"status":"ok","timestamp":1615737010365,"user_tz":-60,"elapsed":856,"user":{"displayName":"Emma Qusit","photoUrl":"","userId":"00282587743940963113"}},"outputId":"461db6cd-fb05-465f-c0df-b4bf0aae6448"},"source":[""],"execution_count":13,"outputs":[{"output_type":"stream","text":["401993\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SKS405cDuOD9"},"source":["## 3. Create n_grams from the text (Optional)"]},{"cell_type":"code","metadata":{"id":"q-cEfiq5StIp","executionInfo":{"status":"ok","timestamp":1615736723982,"user_tz":-60,"elapsed":719,"user":{"displayName":"Emma Qusit","photoUrl":"","userId":"00282587743940963113"}}},"source":["# lemmatizing function\r\n","def lemmatize(text):\r\n","    lemmatizer = WordNetLemmatizer()\r\n","    text = [lemmatizer.lemmatize(word) for word in text]\r\n","    return text\r\n","\r\n","# remove stopwords\r\n","def remove_stopwords(text):\r\n","    stopword = nltk.corpus.stopwords.words('english')\r\n","    text = [word for word in text if word not in stopword]\r\n","    return text\r\n","    \r\n","def tokenize(text):\r\n","    wrong_words = [\"urllink\", \"nbsp\"]\r\n","    tokens = [token for token in text.split(\" \") if token != \"\" and token not in wrong_words]\r\n","    return tokens\r\n","\r\n","def lowered(s):\r\n","    return s.lower()\r\n","\r\n","def remove_nonalph(s):\r\n","      s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\r\n","      return s\r\n","\r\n","def generate_ngrams(s, n):\r\n","    # Use the zip function to help us generate n-grams\r\n","    # Concatentate the tokens into ngrams and return\r\n","    ngrams = zip(*[s[i:] for i in range(n)])\r\n","    return [\" \".join(ngram) for ngram in ngrams]"],"execution_count":21,"outputs":[]},{"cell_type":"code","metadata":{"id":"g2_X-VvcSwY7"},"source":["df[\"text\"] = df[\"text\"].apply(lowered).apply(remove_nonalph).apply(tokenize)\r\n","\r\n","\r\n","# ngram = 2\r\n","# for i, text in enumerate(df['text']):\r\n","#   if type(df['text'].iloc[i]) != list:\r\n","#     lower = lowered(text)\r\n","#     non_alph = remove_nonalph(lower)\r\n","#     tokens = tokenize(non_alph)\r\n","#     # no_stopwords = remove_stopwords(tokens)\r\n","#     # lemma = lemmatize(no_stopwords)\r\n","#     # ngrams = generate_ngrams(lemma, ngram)\r\n","#     df['text'].iloc[i] = tokens\r\n","print(df['text'].iloc[0])\r\n","print(df['text'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jih0FPMqymBD"},"source":["df.index = range(len(df))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YuT6rI7YyqrY"},"source":["df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YzUULzi2Z_Dd"},"source":["# AMOUNT_OF_CATEGORIES = 5\r\n","# df = df.assign(age_group=pd.qcut(df['age'], AMOUNT_OF_CATEGORIES, labels=[i for i in range(AMOUNT_OF_CATEGORIES)]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3IHkERSxa5Vf"},"source":["# hist_trace = go.Histogram(x=df['age_group'])\r\n","# go.Figure(hist_trace).show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-vlT5xX7uSAb"},"source":["## 4. To GPU"]},{"cell_type":"code","metadata":{"id":"cEf8KX5JPkfK"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NFBw7zLxuCee"},"source":["## 5. Initialising Neural Network"]},{"cell_type":"code","metadata":{"id":"Dsg6a_j5mNg7"},"source":["class TextSentiment(nn.Module):\r\n","    def __init__(self, vocab_size, embed_dim, num_class):\r\n","        super().__init__()\r\n","        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\r\n","        self.fc = nn.Linear(embed_dim, num_class)\r\n","        self.init_weights()\r\n","        \r\n","    def init_weights(self):\r\n","        initrange = 0.5\r\n","        self.embedding.weight.data.uniform_(-initrange, initrange)\r\n","        self.fc.weight.data.uniform_(-initrange, initrange)\r\n","        self.fc.bias.data.zero_()\r\n","\r\n","    def forward(self, text, offsets):\r\n","        embedded = self.embedding(text, offsets)\r\n","        return self.fc(embedded)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"X9efH65YV2wi"},"source":["## 6. Classes preprocessing\r\n","Checking the amount of classes and mapping them each to a different unique number "]},{"cell_type":"code","metadata":{"id":"RCFkJq3MtqvE"},"source":["class_choice = 'age_group'\r\n","classdict = defaultdict(int)\r\n","\r\n","for row in df[class_choice]:\r\n","  classdict[row] += 1\r\n","print(sorted(classdict))\r\n","\r\n","### PROBLEM: ALS IK DIT WISSEL NAAR NORMAL SORT DAN IS DE UITKOMST INEENS ANDERS???\r\n","classdict = dict(sorted(classdict.items(), key=operator.itemgetter(1), reverse=True))\r\n","#classdict = dict(sorted(classdict.items()))\r\n","print('classdict:')\r\n","print(classdict)\r\n","\r\n","fig = go.Figure([go.Bar(x=('10s', '20s', '30s'), y=[classdict['10s'], classdict['20s'], classdict['30s']])])\r\n","fig.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RJplh_aSWGbT"},"source":["# class to number mapping\r\n","classlist = list(classdict.keys())\r\n","classmap = dict([(y,x) for x,y in enumerate(classlist)])\r\n","print('classmap:')\r\n","print(classmap)\r\n","print('amount of classes')\r\n","print(len(classdict))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZoTwpYirtto3"},"source":["## 7. Vocabulary dictionary\r\n","Making a dict of all the words in the dataset and mapping each unique word to a unique number"]},{"cell_type":"code","metadata":{"id":"7VP2z-L4nOxd"},"source":["vocabdict = defaultdict(int)   \r\n","\r\n","for row in df['text']:\r\n","  for n_gram in row:\r\n","    n_gram = n_gram.lower()\r\n","    vocabdict[n_gram] += 1\r\n","\r\n","vocabdict = dict(sorted(vocabdict.items(), key=operator.itemgetter(1), reverse=True))\r\n","print(vocabdict)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0T1uELynoUzs"},"source":["# WORD TO NUMBER MAPPING\r\n","l = list(vocabdict.keys())\r\n","wordmap = dict([(y,x) for x,y in enumerate(l)])\r\n","print(wordmap)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kJJoVUa_N1zQ"},"source":["## 8. Mapping text column to numbers (tensor)"]},{"cell_type":"code","metadata":{"id":"1Nse4QPHOBBy"},"source":["df_copy = df.copy()\r\n","\r\n","for i, text in enumerate(df['text']):\r\n","  newtext = []\r\n","  for word in text:\r\n","    word = word.lower()\r\n","    newtext.append(int(wordmap[word]))\r\n","  df['text'][i] = torch.tensor(newtext).to(torch.int64)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GX0r9-j4Rl37"},"source":["train_dataset = []\r\n","\r\n","for index, row in df.iterrows():\r\n","  clasn = classmap[row['age_group']]\r\n","  train_dataset.append(tuple((clasn, row['text'])))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZRwRMsSPbYjA"},"source":["As you can see below the train dataset is now a list with tuples (age, text)"]},{"cell_type":"code","metadata":{"id":"PZAHGOKnTAQo"},"source":["print(train_dataset[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TcONa66LvDFj"},"source":["## 9. Run neural network"]},{"cell_type":"code","metadata":{"id":"e-70LB7smiq4"},"source":["vocab = len(vocabdict)\r\n","embed_dim = 32\r\n","n_classes = len(classdict)\r\n","BATCH_SIZE = 16\r\n","model = TextSentiment(vocab, embed_dim, n_classes).to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Qb4Nn74JvB2R"},"source":["def generate_batch(batch):\r\n","    label = torch.tensor([entry[0] for entry in batch])\r\n","    text = [entry[1] for entry in batch]\r\n","    offsets = [0] + [len(entry) for entry in text]\r\n","    # torch.Tensor.cumsum returns the cumulative sum\r\n","    # of elements in the dimension dim.\r\n","    # torch.Tensor([1.0, 2.0, 3.0]).cumsum(dim=0)\r\n","\r\n","    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\r\n","    text = torch.cat(text)\r\n","    return text, offsets, label"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"F002_I0D0GPs"},"source":["### 9.1 Confusion *matrix*"]},{"cell_type":"code","metadata":{"id":"U4QgicH-0E4O"},"source":["import matplotlib.pyplot as plt\n","from sklearn.metrics import confusion_matrix\n","\n","# confusion matrix plot function\n","def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    import itertools\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    print(cm)\n","\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, format(cm[i, j], fmt),\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.tight_layout()\n","    plt.savefig('/content/drive/MyDrive/Language, speech and dialogue processing/PAN_nn_confusion_matrix.png')\n","    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ijrN2ezY0s4G"},"source":["def train_func(sub_train_):\r\n","\r\n","    # Train the model\r\n","    train_loss = 0\r\n","    train_acc = 0\r\n","    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\r\n","                      collate_fn=generate_batch)\r\n","    for i, (text, offsets, cls) in enumerate(data):\r\n","        optimizer.zero_grad()\r\n","        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\r\n","        output = model(text, offsets)\r\n","        loss = criterion(output, cls)\r\n","        train_loss += loss.item()\r\n","        loss.backward()\r\n","        optimizer.step()\r\n","        train_acc += (output.argmax(1) == cls).sum().item()\r\n","\r\n","    # Adjust the learning rate\r\n","    scheduler.step()\r\n","\r\n","    return train_loss / len(sub_train_), train_acc / len(sub_train_)\r\n","\r\n","def test(data_, last_epoch=False):\r\n","    loss = 0\r\n","    acc = 0\r\n","    predictions = []\r\n","    labels = []\r\n","    inv_map = {v: k for k, v in classmap.items()}\r\n","    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\r\n","    for text, offsets, cls in data:\r\n","        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\r\n","        with torch.no_grad():\r\n","            output = model(text, offsets)\r\n","            loss = criterion(output, cls)\r\n","            loss += loss.item()\r\n","            acc += (output.argmax(1) == cls).sum().item()\r\n","        pred = output.argmax(1).to('cpu')\r\n","        label = cls.to('cpu')\r\n","        predictions.extend(pred)\r\n","        labels.extend(label)\r\n","        \r\n","    if last_epoch == False:\r\n","      return loss / len(data_), acc / len(data_)\r\n","    else:\r\n","      return predictions, labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A-E6whhP0tjq"},"source":["N_EPOCHS = 50\r\n","min_valid_loss = float('inf')\r\n","\r\n","criterion = torch.nn.CrossEntropyLoss().to(device)\r\n","optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\r\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\r\n","\r\n","train_len = int(len(train_dataset) * 0.95)\r\n","sub_train_, sub_valid_ = \\\r\n","    random_split(train_dataset, [train_len, len(train_dataset) - train_len])\r\n","\r\n","start_time = time.time()\r\n","for epoch in range(N_EPOCHS):\r\n","\r\n","  train_loss, train_acc = train_func(sub_train_)\r\n","  valid_loss, valid_acc = test(sub_valid_)\r\n","\r\n","  print('Epoch: %d' %(epoch + 1))\r\n","  print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\r\n","  print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')\r\n","  if epoch == (N_EPOCHS-1):\r\n","    predictions, labels = test(sub_valid_, True)\r\n","    conf_matrix = confusion_matrix(np.array(labels), np.array(predictions), labels=[0, 1, 2])\r\n","    np.set_printoptions(precision=2)\r\n","    # Plot non-normalized confusion matrix\r\n","    #plt.figure()\r\n","    plot_confusion_matrix(conf_matrix, classes=['10s', '20s', '30s'], title='Confusion matrix, without normalization')\r\n","total_time = int(time.time() - start_time)\r\n","print('Total time elapsed: %d seconds.' %(total_time))\r\n","torch.save(model.state_dict(), '/content/drive/MyDrive/Language, speech and dialogue processing/pan_state.pth')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0MQHu4M-vmrq"},"source":["plot_confusion_matrix(conf_matrix, classes=['10s', '20s', '30s'], normalize=True, title='PAN NN Confusion matrix, with normalization')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ShkP2G-KTgZS"},"source":[""],"execution_count":null,"outputs":[]}]}